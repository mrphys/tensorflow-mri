{"cells":[{"cell_type":"markdown","metadata":{},"source":["# 2D Image Segmentation"]},{"cell_type":"markdown","metadata":{"id":"nNiBgvLMBvTb"},"source":["In this tutorial we will be training a 2D UNet to perform automatic segementation of the blood-pool from Cardiac MRI images"]},{"cell_type":"markdown","metadata":{"id":"VkiuEfaI-Kia"},"source":["# Set up TensorFlow MRI\n","If you have not yet installed Pydicom or TensorFlow MRI in your environment you may need to do so now\n","using `pip`"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":92129,"status":"ok","timestamp":1729545275302,"user":{"displayName":"Jennifer Steeden","userId":"01344065473652589669"},"user_tz":-60},"id":"uhZhOxRrAvea","outputId":"597a58df-d735-4922-ed01-0586d7648a56"},"outputs":[],"source":["!pip install tensorflow-mri pydicom"]},{"cell_type":"markdown","metadata":{"id":"Vxt8-2Vs-Kif"},"source":["### Using a GPU\n","TensorFlow MRI supports CPU and GPU computation. If there is a GPU available in your environment and it is visible to TensorFlow, it will be used automatically.\n","\n",":::{tip}\n","In Google Colab, you can enable GPU computation by clicking on\n","**Runtime > Change runtime type** and selecting **GPU** under\n","**Hardware accelerator**.\n",":::\n","\n",":::{tip}\n","You can control whether CPU or GPU is used for a particular operation via\n","the [`tf.device`](https://www.tensorflow.org/api_docs/python/tf/device)\n","context manager.\n",":::"]},{"cell_type":"markdown","metadata":{"id":"d6kWhj8V-Kie"},"source":["Now, import all necessary python libraries."]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":20766,"status":"ok","timestamp":1729545296062,"user":{"displayName":"Jennifer Steeden","userId":"01344065473652589669"},"user_tz":-60},"id":"PZgH6GVs-Kif"},"outputs":[],"source":["from glob import glob\n","import os\n","import pydicom as dicom\n","import tensorflow_mri as tfmri\n","import matplotlib.image as mpimg\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import tensorflow as tf"]},{"cell_type":"markdown","metadata":{"id":"1aja3Djm-Kif"},"source":["### **Prepare the Data**\n","We will be using an open-source cardiac MRI  data set from Sunnybrook Cardiac Data (also known as the 2009 Cardiac MR Left Ventricle Segmentation Challenge data). It consists of 45 cine-MRI images from a mixed of patients and pathologies: healthy, hypertrophy, heart failure with infarction and heart failure without infarction. A subset of this data set was first used in the automated myocardium segmentation challenge from short-axis MRI, held by a MICCAI workshop in 2009, which is what we will be using in this tutorial:\n","\n","*Radau P, Lu Y, Connelly K, Paul G, Dick AJ, Wright GA. “Evaluation Framework for Algorithms Segmenting Short Axis Cardiac MRI.” The MIDAS Journal – Cardiac MR Left Ventricle Segmentation Challenge, http://hdl.handle.net/10380/3070*\n","\n","\n","\n","The  complete data set is  available in the CAP database with public domain license, and can be download from https://www.cardiacatlas.org/sunnybrook-cardiac-data/. Here the manual segmentations are stored as coordinates, however this same data set can be downloaded from Matlab, where the masks have already been created (see: https://uk.mathworks.com/help/deeplearning/ug/cardiac-left-ventricle-segmentation-from-cine-mri-images.html).\n","\n","**Download Data Set**\n","Download the data set from the MathWorks® website and unzip the downloaded folder.\n","\n","```\n","zipFile = matlab.internal.examples.downloadSupportFile(\"medical\",\"CardiacMRI.zip\");\n","filepath = fileparts(zipFile);\n","unzip(zipFile,filepath)\n","The imageDir folder contains the downloaded and unzipped data set.\n","\n","imageDir = fullfile(filepath,\"Cardiac MRI\");\n","```\n","\n","Copy the CardiacMRI folder to your workspace."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18661,"status":"ok","timestamp":1729545314720,"user":{"displayName":"Jennifer Steeden","userId":"01344065473652589669"},"user_tz":-60},"id":"S_6LHJB4Gg-X","outputId":"0a4f3d4b-dee4-4f63-afc8-88c65434109b"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","# specify your image path\n","image_path = '/content/drive/MyDrive/MLtutorials/CardiacMRI/'"]},{"cell_type":"markdown","metadata":{"id":"xhOFKYkdG6iu"},"source":["This dataset consists of short-axis stacks from 45 patients, with manual segmetnations in the LV bloodpool at systole and diastole. The MRI images are in the DICOM file format (.dcm) and the label images are in the PNG file format.\n","\n","First, we need to extract the data and store our image and mask pairs in lists, so that pairs have the same index. Our path names may differ\n","so ensure you the given paths with the path where your data is stored."]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":474488,"status":"ok","timestamp":1729545789204,"user":{"displayName":"Jennifer Steeden","userId":"01344065473652589669"},"user_tz":-60},"id":"DbLptIpi-Kig"},"outputs":[],"source":["# This code works if the data was downloaded from MATLAB. If you took the original data, you will have to process to generate the masks from the coordiates\n","\n","#Create empty lists to store the image and mask data\n","images = []\n","masks = []\n","\n","#Obtain all image and mask data, may need to check the image and mask pathnames to see how they differ\n","\n","# The images (.dcm) files are in the CardiacMRI folder in a folder named 'images' - here they are seperated by patient\n","for filename in glob(os.path.join(image_path,'images/**/*.dcm')):     # This will select all files ending with dcm - these should be your images\n","\n","    #Now we have to find the matching mask data\n","    # These files are instead in a folder names 'labels'\n","    mask_filename = filename.replace('images','labels')                #Replacing the different words in pathnames\n","    # The filenames contain 'gtmask0' in place of '_rawdcm_'\n","    mask_filename_mask = mask_filename.replace('_rawdcm_','gtmask0')\n","    # and they are of type png not dcm\n","    mask_filename_png = mask_filename_mask.replace('dcm','png')        #replacing file type\n","    mask_exists = os.path.exists(mask_filename_png)                    #Checking that this path exists\n","\n","    #If this path exists, we append the images and masks lists so that corresponding images and paths have the same index\n","\n","    if mask_exists:\n","        image = dicom.dcmread(filename).pixel_array\n","        images.append(image)\n","        mask = mpimg.imread(mask_filename_png)\n","        masks.append(mask)\n"]},{"cell_type":"markdown","metadata":{"id":"CiFEudBb-Kig"},"source":["We now have 805 image-mask pairs.\n","\n","Next, we will check if we have assigned the correct pairs of images and masks by plotting them on top of each other. Image cmap convention is 'gray' in MRI, and masks cmap is often 'viridis'. Alpha determines how in focus the mask is (range 0-1, 0.2 is a good choice)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":469},"executionInfo":{"elapsed":514,"status":"ok","timestamp":1729545789713,"user":{"displayName":"Jennifer Steeden","userId":"01344065473652589669"},"user_tz":-60},"id":"M5MXYYjj-Kig","outputId":"32818490-7983-4e9e-f180-0c1b76df0aab"},"outputs":[],"source":["imageNo = 10\n","\n","#testing blended images and masks\n","plt.imshow(images[imageNo],cmap='gray')\n","plt.imshow(masks[imageNo], cmap='viridis',alpha=0.2)\n","plt.title(\"Image With Mask\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1729545789713,"user":{"displayName":"Jennifer Steeden","userId":"01344065473652589669"},"user_tz":-60},"id":"96NzbW42-Kih","outputId":"a9af421c-6bcc-4911-d8d4-01f7f37564ee"},"outputs":[],"source":["#converting to numpy arrays as they are easier to work with\n","masks = np.array(masks)\n","images = np.array(images)\n","\n","# You should check the shape of the data sets.\n","print(f'The shape of the masks is: {masks.shape}')\n","print(f'The shape of the images is: {images.shape}')\n"]},{"cell_type":"markdown","metadata":{"id":"VxI9gFV7-Kih"},"source":["From this we can see we have 805 images and masks, with image size 256x256, as we expect.\n","\n","Next, we need to normalize the image data to so that each data point lies between 0-1."]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":383,"status":"ok","timestamp":1729545790094,"user":{"displayName":"Jennifer Steeden","userId":"01344065473652589669"},"user_tz":-60},"id":"paLV_6sU-Kih"},"outputs":[],"source":["def normalize_images(images):\n","    \"\"\"Normalisation function\n","    Input:\n","           - images, array of image data\n","    Output:\n","           - normalised array of image data\"\"\"\n","\n","    return (images - np.min(images)) / (np.max(images) - np.min(images))\n","\n","#Creating new normalised array\n","images = normalize_images(images)"]},{"cell_type":"markdown","metadata":{"id":"_tLCbdZH-Kii"},"source":["For a 2D Unet model, we need our mask data to be seperated in two classes, in this case background and bloodpool. All data points equal to 0 we can keep as zero, and all non zero points we will make equal to 1."]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1729545790094,"user":{"displayName":"Jennifer Steeden","userId":"01344065473652589669"},"user_tz":-60},"id":"AciwAsRN-Kii"},"outputs":[],"source":["#simplifying masks\n","def simplify_mask(masks):\n","    \"\"\"This function take an inputed array of masks and keeps leaves all zero data points as zero\n","    and converts all non-zero data points equal to one. Zero data point are equivalent to the background\n","    and those equal to one represent the bloodpool\n","\n","    Input: masks, array containing mask data\n","\n","    Output: simp_masks, array containing the simplified masks\"\"\"\n","\n","    simp_masks = np.zeros_like(masks, dtype=np.float32)\n","    simp_masks[masks ==0] = 0        #background\n","    simp_masks[masks !=0] = 1        #bloodpool\n","\n","    return simp_masks"]},{"cell_type":"markdown","metadata":{"id":"UVRSo7Ky-Kii"},"source":["We can now apply this function to our `masks` array, and do a quick check of the minimum and maximum values (which should be 0 and 1 respectively), and plot one of the simplified masks."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":452},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1729545790094,"user":{"displayName":"Jennifer Steeden","userId":"01344065473652589669"},"user_tz":-60},"id":"9DwzTu0h-Kii","outputId":"55ffe8f2-e47c-4060-9820-2e4403d77bea"},"outputs":[],"source":["#Simplifying all masks\n","masks = simplify_mask(masks)\n","\n","#Test\n","print(f\"(min, max): ({masks.min()}, {masks.max()})\")\n","\n","plt.imshow(masks[0], cmap=\"viridis\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"iwox3FFm-Kij"},"source":["Next, we need to seperate our data into training, testing and validation data. We will use the ratio of 80%, 10%, 10% respectively, although this is some what arbitrary. Ideally you should use as much of the data as possible for training, while keeping enough for testing and validation."]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1729545790094,"user":{"displayName":"Jennifer Steeden","userId":"01344065473652589669"},"user_tz":-60},"id":"ZYlJPJAs-Kij"},"outputs":[],"source":["#seperating into training, testing and validation data\n","\n","#Finding the number of images/masks in category\n","n_train = int(0.8*masks.shape[0])                         #80%\n","n_test =int(0.1*masks.shape[0])                           #10%\n","n_val = int(0.1*masks.shape[0] )                           #10%\n","\n","#Note: rounded we will have 81 test data and 80 val (not exactly 10%)\n","\n","#seperating the data into categories\n","\n","train_images = images[:n_train]\n","train_masks = masks[:n_train]\n","\n","test_images = images[n_train:n_train+n_test+1]\n","test_masks = masks[n_train:n_train+n_test+1]\n","\n","val_images = images[n_train+n_test+1:]\n","val_masks = masks[n_train+n_test+1:]"]},{"cell_type":"markdown","metadata":{"id":"0A7MxzTE-Kij"},"source":["We need to add a \"channel dimension\" to have our data in the correct format for tensorflow. We can do this by using the `np.expand_dims()` function. Set axis = 3"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1729545790094,"user":{"displayName":"Jennifer Steeden","userId":"01344065473652589669"},"user_tz":-60},"id":"u_TxF-yf-Kij"},"outputs":[],"source":["train_images = np.expand_dims(train_images,axis=3)\n","test_images = np.expand_dims(test_images,axis=3)\n","val_images = np.expand_dims(val_images,axis=3)\n","\n","train_masks = np.expand_dims(train_masks,axis=3)\n","test_masks = np.expand_dims(test_masks,axis=3)\n","val_masks = np.expand_dims(val_masks,axis=3)"]},{"cell_type":"markdown","metadata":{"id":"PrO5vao--Kij"},"source":["We now need to set our parameters for the model"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1729545790094,"user":{"displayName":"Jennifer Steeden","userId":"01344065473652589669"},"user_tz":-60},"id":"Lxexijjo-Kik"},"outputs":[],"source":["IM_height = images.shape[1]     #Extracting the image height\n","IM_length = images.shape[2]     #Extracting the image length\n","channels = 1                    #Only need 1 input channel so set to 1\n","batch_size = 16                 #Arbitrary choice, you can try setting to different values to optimize the model\n","epochs = 100                     #ideally set to 100, depending on time and size of GPU. The larger the number of epochs the better, however increases execution time"]},{"cell_type":"markdown","metadata":{"id":"kVz3Wuhk-Kik"},"source":["We now need to convert our data into tensorflow (tf) datasets, and then seperate them into batches."]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1729545790399,"user":{"displayName":"Jennifer Steeden","userId":"01344065473652589669"},"user_tz":-60},"id":"f9lS0ecR-Kik"},"outputs":[],"source":["train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_masks))\n","train_dataset = train_dataset.batch(batch_size)\n","\n","val_dataset = tf.data.Dataset.from_tensor_slices((val_images, val_masks))\n","val_dataset = val_dataset.batch(batch_size)\n","\n","test_dataset = tf.data.Dataset.from_tensor_slices((test_images))\n","test_dataset = test_dataset.batch(batch_size)"]},{"cell_type":"markdown","metadata":{"id":"EnRYJSy6-Kik"},"source":["The next cell is optional, however if you would like to save you model so that it can be used at a later date it is quite useful.\n","The code below will allow the model automatically save the best model. While the model is being build, it will only save itself if the validation loss has decreased compared to the model at any previous epoch."]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":274,"status":"ok","timestamp":1729545790672,"user":{"displayName":"Jennifer Steeden","userId":"01344065473652589669"},"user_tz":-60},"id":"W-OLoIpz-Kik"},"outputs":[],"source":["#First create a pathname of where to save the model\n","checkpointpath = \"./models/unet_segmentation/model.ckpt\"\n","checkpoint_dir = os.path.dirname(checkpointpath)\n","\n","#\n","cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpointpath, save_best_only=True, verbose=1)"]},{"cell_type":"markdown","metadata":{"id":"X0TWElUP-Kik"},"source":["### Building Our Model\n","Now we are ready to make our model. TensorFlow MRI has a premade 2D Unet model function which can be called from\n","`tfmri.models.Unet2D()`. Details of this model and its unputs can be found on the TensorFlow MRI website.\n","The basic parameters we need to define are:\n","\n","`filters=` - The number of filters for convolutional layers at each scale. The number of scales is inferred as len(filters). E.g. a list of 3 values/scales, where each value is double that of the last eg. `[32,64,128]`\n","\n","`kernel_size=` - define the dimensions of the kernel as a list, if you only input 1 value then it will assume dimensions are the same size\n","\n","`out_channels=` - should keep the same as the number of input channels, in this case =1\n","\n","`out_activation=` - set this = `\"sigmoid\"` for segmentation task\n","\n","\n","We then need to compile our model. The basic parameters we need to define are:\n","\n","`optimizer=` - normally set our optimizer = `\"adam\"`, more options available on [GitHub](https://mrphys.github.io/tensorflow-mri/)\n","\n","`loss=` - many options which you can read about on [GitHub](https://mrphys.github.io/tensorflow-mri/), however we will use `\"binary_crossentropy\"`. You can test and\n","compare some of the other loss types.\n","\n","`metrics=` - we will use `tfmri.metrics.Accuracy()`"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1729545790672,"user":{"displayName":"Jennifer Steeden","userId":"01344065473652589669"},"user_tz":-60},"id":"PEHklKFV-Kik"},"outputs":[],"source":["model = tfmri.models.UNet2D(filters= [32,64,128], kernel_size = 3, out_channels=1, out_activation = \"sigmoid\")\n","model.compile(optimizer = 'adam', loss=\"binary_crossentropy\", metrics=tfmri.metrics.Accuracy())"]},{"cell_type":"markdown","metadata":{"id":"JDxRgIVQ-Kil"},"source":["Now we will build our model to our data using the `fit()` function.\n","input:\n","\n","`training_data=` - we use our `train_dataset`\n","\n","`validation_data=` - we use our `val_dataset`\n","\n","`batch_size=` - we use our `batch_size`\n","\n","`verbose=` - we set = 1\n","\n","`epochs=` - we use our `epochs`\n","\n","`callbacks=` - this is what saves our model, we set to our variable `cp_callback`\n","\n","Building our model will take some time  so you may want to test that it is working by first using a small number of epochs (even just 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zakuaTUw-Kil","outputId":"f3cc3dc5-d82e-407f-8683-b3dbc7c58fa3"},"outputs":[],"source":["history = model.fit(train_dataset,\n","                    validation_data=val_dataset,\n","                    batch_size=batch_size,\n","                    verbose=1,\n","                    epochs=epochs,\n","                    callbacks=[cp_callback])"]},{"cell_type":"markdown","metadata":{"id":"G-qBJIbb-Kil"},"source":["### Results\n","Our model should now be build, we can extract the loss and accuracy data and plot to see how they very accross epochs."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bBRSSM5H-Kil"},"outputs":[],"source":["#Exctracting data from our model\n","loss = history.history['loss']\n","accuracy = history.history['acc']\n","epochs = np.arange(1,len(loss)+1)\n","\n","loss_range = len(loss)\n","val_loss = history.history['val_loss']\n","\n","#Creating figures and plotting our losses and accuracy against epochs.\n","fig = plt.figure(figsize=(20,15))\n","ax1, ax2 = plt.subplot(2,2,1), plt.subplot(2,2,2)\n","ax1.plot(epochs,loss,'b',label= 'Loss')\n","ax1.plot(epochs,val_loss,'g', label = 'Validation Loss')\n","ax1.legend()\n","ax1.set_title('Cross Entropy Loss', fontsize='16')\n","ax1.set_xlabel('Epochs', fontsize = '14')\n","ax1.set_ylabel('Loss', fontsize = '14')\n","\n","ax2.plot(epochs,accuracy,'r')\n","ax2.set_title('Accuracy', fontsize = '16')\n","ax2.set_xlabel('Epochs', fontsize = '14')\n","ax2.set_ylabel('Accuracy', fontsize = '14')"]},{"cell_type":"markdown","metadata":{"id":"RDrz54E1-Kim"},"source":["We can see that the losses decrease accross the epochs, and the accuracy increases.\n","\n","Now we can apply our model to the `test_dataset` to inspect if the model is actual able to predict the masks of our test images."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RwDBg21u-Kim"},"outputs":[],"source":["#Predict the masks of our test images\n","out_ims = model.predict(test_dataset)"]},{"cell_type":"markdown","metadata":{"id":"nXL_zYb5-Kim"},"source":["We can now plot the ground truth test image and mask (the image with its premade mask) next to the image and generated mask to compare."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e12Mj3c7-Kin"},"outputs":[],"source":["i = 9\n","#pltting the above image index\n","\n","figcompare = plt.figure(figsize=(16,12))\n","ax1, ax2, ax3, ax4 = plt.subplot(2,4,1), plt.subplot(2,4,2), plt.subplot(2,4,3), plt.subplot(2,4,4)\n","\n","ax1.imshow(test_images[i],cmap='gray')\n","ax1.imshow(test_masks[i],alpha=0.2)\n","ax1.set_title(\"Test Image with Ground Truth Mask\")\n","\n","ax2.imshow(test_images[i], cmap='gray')\n","ax2.imshow(out_ims[i],alpha=0.2)\n","ax2.set_title(\"Test Image with Predicted Mask\")\n","\n","ax3.imshow(test_masks[i])\n","ax3.set_title('Ground Truth Mask')\n","\n","ax4.imshow(out_ims[i])\n","ax4.set_title('Predicted Mask')\n"]},{"cell_type":"markdown","metadata":{"id":"JbNfbvd6-Kin"},"source":["We can also plot the difference map between the ground truth mask and model generated mask"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oLK-iYdb-Kin"},"outputs":[],"source":["#Difference in Mask\n","mask_diff = abs(out_ims[i] - test_masks[i])\n","\n","plt.imshow(mask_diff)\n","plt.title(\"Difference Map\")"]},{"cell_type":"markdown","metadata":{"id":"VFyGnFsd-Kio"},"source":["# Conclusion\n","Congratulations! You've performed image segmentation using TensorFlow MRI. The code in this notebook will work with any 2D dataset, so feel free to use your own. You can also continue to test this model using different parameters, which can be found on [GitHub](https://github.com/mrphys/tensorflow-mri/issues/new)."]},{"cell_type":"markdown","metadata":{"id":"FmT8dB7O-Kio"},"source":["### Let us know!\n","Please tell us what you think about this tutorial and about TensorFlow MRI.\n","We would like to hear what you liked and how we can improve. You will find us\n","on [GitHub](https://mrphys.github.io/tensorflow-mri/)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6qAnNz6C-Kio"},"outputs":[],"source":["# Copyright 2022 University College London. All rights reserved.\n","#\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","#     https://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License."]}],"metadata":{"accelerator":"TPU","colab":{"gpuType":"V28","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":0}
